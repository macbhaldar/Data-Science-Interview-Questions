## Basic Questions

### What does one understand by the term Data Science?
An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science.
- It starts with gathering the business requirements and relevant data.
- Once the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture.
- Data processing does the task of exploring the data, mining it, analyzing it which can be finally used to generate the summary of the insights extracted from the data.
- Once the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements.
- In the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture.

###  Describe 'Training set' and 'training Test'.
In various areas of information of machine learning, a set of data is used to discover the potentially predictive relationship, which is known as 'Training Set'. The training set is an example that is given to the learner. Besides, the 'Test set' is used to test the accuracy of the hypotheses generated by the learner. It is the set of instances held back from the learner. Thus, the training set is distinct from the test set.

### State the difference between a Validation Set and a Test Set
A Validation set mostly considered as a part of the training set as it is used for parameter selection which helps you to avoid overfitting of the model being built.
While a Test Set is used for testing or evaluating the performance of a trained machine learning model.

### When is resampling done?
Resampling is a methodology used to sample data for improving accuracy and quantify the uncertainty of population parameters. It is done to ensure the model is good enough by training the model on different patterns of a dataset to ensure variations are handled. It is also done in the cases where models need to be validated using random subsets or when substituting labels on data points while performing tests.

### Why do you need to perform resampling?
Resampling is done in below-given cases:
- Estimating the accuracy of sample statistics by drawing randomly with replacement from a set of the data point or using as subsets of accessible data
- Substituting labels on data points when performing necessary tests
- Validating models by using random subsets

### What is reinforcement learning?
Reinforcement Learning is a learning mechanism about how to map situations to actions. The end result should help you to increase the binary reward signal. In this method, a learner is not told which action to take but instead must discover which action offers a maximum reward. As this method based on the reward/penalty mechanism.

### What is Back Propagation?
Back-propagation is the essence of neural net training. It is the method of tuning the weights of a neural net depend upon the error rate obtained in the previous epoch. Proper tuning of the helps you to reduce error rates and to make the model reliable by increasing its generalization.

## Statistical Analysis

### What are some of the techniques used for sampling? What is the main advantage of sampling?
Data analysis can not be done on a whole volume of data at a time especially when it involves larger datasets. It becomes crucial to take some data samples that can be used for representing the whole population and then perform analysis on it. While doing this, it is very much necessary to carefully take sample data out of the huge data that truly represents the entire dataset.
There are majorly two categories of sampling techniques based on the usage of statistics, they are:
- **Probability Sampling techniques:** Clustered sampling, Simple random sampling, Stratified sampling.
- **Non-Probability Sampling techniques:** Quota sampling, Convenience sampling, snowball sampling, etc.

### List down the conditions for Overfitting and Underfitting.
- **Overfitting:** The model performs well only for the sample training data. If any new data is given as input to the model, it fails to provide any result. These conditions occur due to low bias and high variance in the model. Decision trees are more prone to overfitting.
- **Underfitting:** Here, the model is so simple that it is not able to identify the correct relationship in the data, and hence it does not perform well even on the test data. This can happen due to high bias and low variance. Linear regression is more prone to Underfitting.

###  What is Collaborative filtering?
Collaborative filtering used to search for correct patterns by collaborating viewpoints, multiple data sources, and various agents.

### Define the term cross-validation
Cross-validation is a validation technique for evaluating how the outcomes of statistical analysis will generalize for an Independent dataset. This method is used in backgrounds where the objective is forecast, and one needs to estimate how accurately a model will accomplish.

### State the difference between the expected value and mean value?
There are not many differences between these two, but it is to be noted that these are used in different contexts. The mean value generally refers to the probability distribution whereas the expected value is referred to in the contexts involving random variables.

### Explain the benefits of using statistics by Data Scientists
Statistics help Data scientist to get a better idea of customer’s expectation. Using the statistic method Data Scientists can get knowledge regarding consumer interest, behavior, engagement, retention, etc. It also helps you to build powerful data models to validate certain inferences and predictions.

### What is Normal Distribution?
A normal distribution is a set of a continuous variable spread across a normal curve or in the shape of a bell curve. You can consider it as a continuous probability distribution which is useful in statistics. It is useful to analyze the variables and their relationships when we are using the normal distribution curve.

### What is p-value?
When you conduct a hypothesis test in statistics, a p-value allows you to determine the strength of your results. It is a numerical number between 0 and 1. Based on the value it will help you to denote the strength of the specific result.

### What do you understand by the F1 score?
The F1 score represents the measurement of a model's performance. It is referred to as a weighted average of the precision and recall of a model. The results tending to 1 are considered as the best, and those tending to 0 are the worst. It could be used in classification tests, where true negatives don't matter much.

### What is skewed Distribution & uniform distribution?
Skewed distribution occurs when if data is distributed on any one side of the plot whereas uniform distribution is identified when the data is spread is equal in the range.

### What is precision?
Precision is the most commonly used error metric is n classification mechanism. Its range is from 0 to 1, where 1 represents 100%.

###  What are the Eigenvalue and Eigenvector?
Eigenvectors are for understanding linear transformations. Data scientist need to calculate the eigenvectors for a covariance matrix or correlation. Eigenvalues are the directions along using specific linear transformation acts by compressing, flipping, or stretching.

###  Is it possible to capture the correlation between continuous and categorical variable?
Yes, we can use analysis of covariance technique to capture the association between continuous and categorical variables.

## Regression

### What is a Linear Regression?
Linear regression is a statistical programming method where the score of a variable ‘X’ is predicted from the score of a second variable ‘Y’. Y is referred to as the predictor variable and X as the criterion variable.

### Name three disadvantages of using a linear model
Three disadvantages of the linear model are:
1. The assumption of linearity of the errors.
2. You can’t use this model for binary or count outcomes
3. There are plenty of overfitting problems that it can’t solve

### What is logistic regression in Data Science?
Logistic Regression is also called as the logit model. It is a method to forecast the binary outcome from a linear combination of predictor variables.

Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.

### What are the Logistic Regression Assumptions?
- Binary logistic regression requires the dependent variable to be binary.
- For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.
- Only the meaningful variables should be included.
- The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.
- The independent variables are linearly related to the log odds.
- Logistic regression requires quite large sample sizes.

### What is SMOTE?
SMOTE (Synthetic Minority Oversampling Technique) is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.

### What is Recursive Feature Elimination?
Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.

### What are precision, recall, F-measure and support and how to compute them?
- Precision : The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.
- REcall : The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.
- F-beta : The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.
- Support : The support is the number of occurrences of each class in y_test.
```
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```

### What is ROC curve?
The receiver operating characteristic (ROC) curve is common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).

### Difference between Regression and Classification?
The major difference between Regression and Classification is that Regression results in a continuous quantitative value while Classification is predicting the discrete labels.
However, there is no clear line that draws the difference between the two. We have a few properties of both Regression and Classification. 
These are as follows:

**Regression**
- Regression predicts the quantity.
- We can have discrete as well as continuous values as input for regression.
- If input data are ordered with respect to the time it becomes time series forecasting.

**Classification**
- The Classification problem for two classes is known as Binary Classification.
- Classification can be split into Multi- Class Classification or Multi-Label Classification.
- We focus more on accuracy in Classification while we focus more on the error term in Regression.

### What is Power Analysis?
The power analysis is an integral part of the experimental design. It helps you to determine the sample size requires to find out the effect of a given size from a cause with a specific level of assurance. It also allows you to deploy a particular probability in a sample size constraint.

### What is bias?
Bias is an error introduced in your model because of the oversimplification of a machine learning algorithm.” It can lead to underfitting.

### What is ‘Naive’ in a Naive Bayes algorithm?
The Naive Bayes Algorithm model is based on the Bayes Theorem. It describes the probability of an event. It is based on prior knowledge of conditions which might be related to that specific event.

### Name three types of biases that can occur during sampling
In the sampling process, there are three types of biases, which are:
1. Selection bias
2. Under coverage bias
3. Survivorship bias

### What do you know about Bayesian Networks?
Bayesian Networks also referred to as 'belief networks' or 'casual networks', are used to represent the graphical model for probability relationship among a set of variables.
For example, a Bayesian network can be used to represent the probabilistic relationships between diseases and symptoms. As per the symptoms, the network can also compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference or learning in Bayesian networks. Bayesian networks which relate the variables (e.g., speech signals or protein sequences) are called dynamic Bayesian networks.

### Which are the two components of Bayesian logic program?
A Bayesian logic program consists of two components:
- **Logical** - It contains a set of Bayesian Clauses, which capture the qualitative structure of the domain.
- **Quantitative** - It is used to encode quantitative information about the domain.

### Explain cluster sampling technique in Data science
A cluster sampling method is used when it is challenging to study the target population spread across, and simple random sampling can’t be applied.

### Explain why Data Cleansing is essential and which method you use to maintain clean data
Dirty data often leads to the incorrect inside, which can damage the prospect of any organization. For example, if you want to run a targeted marketing campaign. However, our data incorrectly tell you that a specific product will be in-demand with your target audience; the campaign will fail.

### When underfitting occurs in a static model?
Underfitting occurs when a statistical model or machine learning algorithm not able to capture the underlying trend of the data.

## Classifier Models

### What is Decision Tree algorithm?
A decision tree is a popular supervised machine learning algorithm. It is mainly used for Regression and Classification. It allows breaks down a dataset into smaller subsets. The decision tree can able to handle both categorical and numerical data.

###  What is a Random Forest?
Random forest is a machine learning method which helps you to perform all types of regression and classification tasks. It is also used for treating missing values and outlier values.

### What is the K-means clustering method?
K-means clustering is an important unsupervised learning method. It is the technique of classifying data using a certain set of clusters which is called K clusters. It is deployed for grouping to find out the similarity in the data.

### What is the term Binomial Probability Formula?
The binomial distribution contains the probabilities of every possible success on N trials for independent events that have a probability of π of occurring.

## Bagging and Boosting

### What is Bagging and Boosting?
- **Bagging** is a process in ensemble learning which is used for improving unstable estimation or classification schemes.
- **Boosting** methods are used sequentially to reduce the bias of the combined model.

### What are the similarities between bagging and boosting in Machine Learning?
- Both are the ensemble methods to get N learns from 1 learner.
- Both generate several training data sets with random sampling.
- Both generate the final result by taking the average of N learners.
- Both reduce variance and provide higher scalability.

### What are the differences between bagging and boosting in Machine Learning?
- Although they are built independently, but for Bagging, Boosting tries to add new models which perform well where previous models fail.
- Only Boosting determines the weight for the data to tip the scales in favor of the most challenging cases.
- Only Boosting tries to reduce bias. Instead, Bagging may solve the problem of over-fitting while boosting can increase it.
